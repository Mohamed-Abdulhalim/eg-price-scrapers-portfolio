name: Run scrapers daily

on:
  schedule:
    - cron: "0 21 * * *"
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180     # <— extended limit (3 hours)

    concurrency:
      group: eg-scrape
      cancel-in-progress: false   # <— prevents daily overlap kills

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Debug env presence (no secrets printed)
        run: |
          [ -n "$SUPABASE_URL" ] && echo "SUPABASE_URL present" || (echo "SUPABASE_URL MISSING"; exit 1)
          [ -n "$SUPABASE_SERVICE_ROLE_KEY" ] && echo "SERVICE_ROLE present" || (echo "SERVICE_ROLE MISSING"; exit 1)

      - name: Install Chrome (optional)
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable || true

      # ---------------------- Your existing scrapers ----------------------

      - name: Run Amazon scraper
        run: |
          echo -e "mobiles\niphone 13" | python scrapers/_amazon.py

      - name: Run Noon scraper
        run: |
          echo -e "iphone 13\nmobiles" | python scrapers/noon.py

      - name: Run Jumia scraper
        run: |
          echo -e "iphone 13\nmobiles" | python scrapers/jumia.py

      - name: Run BTech scraper
        run: |
          echo -e "iphone 13\nmobiles" | python scrapers/btech.py

      - name: Run 2B scraper (with sticky EG proxy)
        timeout-minutes: 170    # <— optional per-step timeout
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPER_PROXY: ${{ secrets.SCRAPER_PROXY }}
          NO_PROXY: ${{ secrets.NO_PROXY }}
          PRINT_PROXY_IP: "1"
        run: |
          python scrapers/_2b.py --lang ar --max-pages 10
