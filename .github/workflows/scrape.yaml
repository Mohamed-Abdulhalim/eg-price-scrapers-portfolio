name: Run scrapers daily

on:
  schedule:
    - cron: "0 2 * * *"   # every day at 02:00 UTC
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    env:
      # shared across steps
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Debug env presence (no secrets printed)
        run: |
          [ -n "$SUPABASE_URL" ] && echo "SUPABASE_URL present" || (echo "SUPABASE_URL MISSING"; exit 1)
          [ -n "$SUPABASE_SERVICE_ROLE_KEY" ] && echo "SERVICE_ROLE present" || (echo "SERVICE_ROLE MISSING"; exit 1)

      # Chrome â€” harmless if unused
      - name: Install Chrome (optional)
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable || true

      # ---------------------- Your existing scrapers ----------------------

      - name: Run Amazon scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          echo -e "mobiles\niphone 13" | python scrapers/_amazon.py

      - name: Run Noon scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          echo -e "iphone 13\nmobiles" | python scrapers/noon.py

      - name: Run Jumia scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          echo -e "iphone 13\nmobiles" | python scrapers/jumia.py

      - name: Run BTech scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          echo -e "iphone 13\nmobiles" | python scrapers/btech.py

      # ---------------------- 2B with proxy ----------------------

      - name: Run 2B scraper (with sticky EG proxy)
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPER_PROXY: ${{ secrets.SCRAPER_PROXY }}     # e.g. http://login:pass@gw.dataimpulse.com:10000
          NO_PROXY: ${{ secrets.NO_PROXY }}               # e.g. .supabase.co,localhost,127.0.0.1
          PRINT_PROXY_IP: "1"                             # optional: print outbound IP once
        run: |
          python scrapers/_2b.py --lang ar --max-pages 10
